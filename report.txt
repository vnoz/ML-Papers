***Context

Multimodal learning is a branch of machine learning that develops models from multiple data types known as modalities. These modalities might be from text, image, audio, video, and sensor data. Multimodal learning offers several key advantages, including the creation of richer feature representations, the development of a more comprehensive contextual understanding, and the enhanced robustness and generalization of the resulting model. In healthcare domain, by integrating data from medical images, health records, and genomics, multimodal learning provides a holistic view of the patient, enhancing diagnosis and clinical practice in a way that mirrors a physician's comprehensive approach.\par
        
However, integrating multiple modalities presents significant challenges. According to \cite{baltruvsaitis2018multimodal} and \cite{liang2024foundations}, these challenges can be categorized into core technical areas such as representation, alignment, fusion, generation, reasoning, and quantification. Processing data input from different data types (e.g. medical image and associated clinical report) into a compatible and useful form to train the model, aligning corresponding elements across modalities (e.g., matching words to image regions), and the fusion of these features effectively are crucial tasks toward an effective solution. Multimodal learning in healthcare faces unique difficulties arising from the nature of medical data and the complexity of clinical tasks. For instance, in vision modality, specific anatomical regions may contain distinct diagnostic meanings for spatial disease assessment, while the variations in density of pathology sections can indicate differences in disease severity for temporal disease assessment. In language modality, medical reports often contain negations—such as "there is no evidence of disease"—which are uncommon in general-purpose text. These constructions present a unique challenge for natural language processing models, complicating contextual understanding. Therefore, a critical prerequisite for efficient model training is the processing step to transform multimodal healthcare data into useful and compatible representations, which involves the precise alignment of corresponding elements across different modalities.\par

Representation learning has the primary goal of automatically discover optimal representations or features from raw data that contains useful information for classifiers or other downstream tasks. For multimodal data, alongside the traditional supervised and unsupervised paradigms, modern representation learning increasingly employs self-supervised learning (SSL) approach. SSL generates supervision signals from the inherent structure or patterns within the data itself, rather than relying on labelled data. For instance, data from multiple modalities can be treated as a positive pair if they originate from the same instance, or as a negative pair if they are shuffled between instances. SSL is particularly suitable for multimodal representation learning in healthcare due to the high informational content of medical images and the strong correlation between radiology report descriptions and the anatomical structures visible in medical imaging.\par

Multimodal representation learning can be grouped into two categories: joint and coordinated (\cite{baltruvsaitis2018multimodal}). Joint representation learning processes and transforms multimodal data (like text and images) into a single, unified representation space. In contrast, coordinated representation learning processes each data type separately but aligns their individual representations through similarity constraints, creating a "coordinated space" where related concepts from different modalities are projected close together

****Problem statement

Multimodal learning in healthcare, by integrating diverse multimodal data sources, such as medical images, EHRs, and clinical reports, offers many potentials to enhance clinical practices and diagnostic support systems. In an ideal paradigm, multimodal learning aims to create meaningful representations from heterogeneous sources, establish robust cross-modal alignments, and fuse these representations into a cohesive and information-rich embedding for downstream tasks. While generating modality-specific representations and fusing them are well-established processes, in the sense that we can select proper architect with available model from toolbox and use standard fusion methods to fuse them, the precise cross-modal alignment between components and concepts remains a substantially more difficult task and is often required domain-specific design. Achieving semantic alignment between multiple modalities poses a significant challenge for multimodal models in healthcare, primarily due to the high dimensionality of medical imaging data and the complex, often hidden, correlations of sub-components between different data types.\par

Common multimodal alignment approaches have two key aspects with potential for improvement. First, while many methods align either global features (e.g. entire images and texts) or local features (e.g. sub-regions and sentences), few architectures integrate both levels of alignment. Second, the field is predominantly focused on bimodal vision-language pairs, and very few models are capable of handling more than two modalities.\par

This thesis proposes an information-theoretic framework to address these challenges of multimodal alignment in healthcare. The framework is primarily based on Mutual Information, a cornerstone of information theory that quantifies the shared information between random variables. Using the core idea of maximizing Mutual Information, models can be guided to align salient features across modalities. While the direct estimation of MI has historically limited its application, recent advances in neural estimators—such as MINE and InfoNCE—make it a practical training objective. Building on this foundation, the proposed research will employ a unified approach utilizing MI, its multivariate generalization Total Correlation for scenarios involving more than two modalities, and complementary similarity measures for fine-grained alignment.

*** Research questions

In this research, we will mainly investigate two main questions:
1. How to align and incorporate the local futures in each modality with the global features to form better embeddings. 
2. How to extend MI frameworks for multimodal alignment beyond 2 modalities.

In multimodal healthcare, global features are derived from an entire data modality, providing a high-level summary, such as an embedding for a full medical image or a complete clinical report, while local features capture fine-grained information from sub-components, such as specific regions in an image or individual clinical concepts within a text. The rationale for aligning and integrating both local and global features is from the observation of the actual diagnostic process of physicians, who typically read a report, associate key textual findings with specific visual evidence, and combine this information with other data to reach a final conclusion. In an ideal of perfect representation learning, local features would highlight localized pathological findings, while global features would provide the broader anatomical and clinical context. Therefore, the alignment of both feature levels is essential for constructing a comprehensive and clinical relevant multimodal representation.

Common multimodal alignment approaches, such as contrastive learning and transformer-based cross-modal attention, were capable to handle bimodal data, and in healthcare, this has predominantly manifested in Vision-Language models designed for image-text pairs. A comprehensive clinical diagnosis, however, requires the integration of diverse data sources, extending beyond the scope of medical images and radiology reports. Extending these frameworks to accommodate more than two modalities is doable but often requires considerable adaptation, for example, using intra-modal and inter-modal contrastive learning (\cite{yuan2021multimodal}), Hierarchical Attention (\cite{xu2023multimodal}), InterBERT(\cite{lin2020interbert}). These extensions can be non-trivial and may lack a rigorous theoretical foundation. In contrast, information theory has the concept of Total Correlation, a generalization of Mutual Information, to quantify the shared dependency among multiple random variables. Consequently, constructing a multimodal alignment framework upon the established principles of MI and Total Correlation is a logical and promising research direction with strong theoretical background.

*** Expected contributions

This work targets two gaps in multimodal learning: the rare combination of global and local feature alignment, and a predominant focus on bimodal data. Our expected contributions are: (1) to introduce a novel theoretical framework that jointly optimizes global and local alignment objectives and (2) to create a generalized framework for semantic alignment across three or more modalities.


----------Chapter 2: Background and related works
*** Common approaches for multimodal alignment
Current multimodal models primarily handle alignment through two dominant mechanisms: contrastive-based alignment and transformer-based cross-attention. Contrastive methods, with notable models such as CLIP (OpenAI, \cite{radford2021learning}) for general vision-language tasks and ConVIRT (\cite{zhang2022contrastive}) for medical data, project entire images and text descriptions into a shared latent space. These models are then optimized using a contrastive objective that pulls matched pairs together while pushing non-matches apart. Conversely, transformer-based models leverage cross-modal attention to achieve fine-grained, local alignment. These models tokenize sub-regions of images and words or phrases in text, align representations from one modality to relevant components in the other.

*** Mutual Information and the generalization for multi-variables
Recently, Mutual Information has been used as a training criterion to deliver noticeable performance improvement for deep models on various learning tasks, but calculating Mutual Information values is challenging, especially when the closed-form distributions of variables are unknown, so the common approaches are using sample-based estimators with the bound:
1. Mutual Information Neural Estimator (MINE): based on K-L divergen
2. Noise Contrastive Estimation (NCE): based on a group of samples to obtain a low-variance estimator

*** Multimodal alignment with global and local features

*** Multimodal alignment with more than two modalities